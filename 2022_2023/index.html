<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html;charset=UTF-8" />
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description"
    content="EgoVis" />
  <meta name="keywords" content="video dataset, machine perception, machine-learning, research, egocentric videos, ego4d, epic-kitchens" />

  <title>EgoVis 2022/2023 Distinguished Paper Awards</title>
  <link rel="preconnect" href="https://use.fontawesome.com" />
  <link rel="preconnect" href="https://maxcdn.bootstrapcdn.com" />
  <link rel="preconnect" href="https://ajax.googleapis.com" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" />
  <link rel="preconnect" href="https://assets.ego4d-data.org" />

  <link rel="stylesheet" href="../../assets/css/style.css" />

  <link rel="preload" as="image" href="../../assets/images/ego-4d-01.png" />
  <link rel="preload" as="image" href="../--/assets/images/map.jpg"
    media="screen and (max-width: 1024px), (max-height:511px)" />

  <link rel="icon" href="../../assets/images/egovis-logo.ico" />

  <!-- Latest font-awesome css for icons -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css"
    integrity="sha384-DyZ88mC6Up2uqS4h/KRgHuoeGwBcD4Ng9SiP4dIRy0EXTlnuz47vAwmeGwVChigm" crossorigin="anonymous" />

  <!-- BS3 compiled and minified CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" />

  <!-- jQuery library -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <!-- Latest compiled JavaScript for BS3 -->
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

  <script src="../../assets/scripts/form-submission.js"></script>

  <script type="text/javascript">
    function openTabbedContent(element) {
      var whereTo = $(element).attr("goto"),
        tabs = $("#benchmarks li"),
        links = tabs.find("a[href='#" + whereTo + "']");

      links[0].click();
    }
    // Hide the address bar
    // https://24ways.org/2011/raising-the-bar-on-mobile/
    // https://github.com/scottjehl/Hide-Address-Bar
    /*
     * Normalized hide address bar for iOS & Android
     * (c) Scott Jehl, scottjehl.com
     * MIT License
     */
    (function (win) {
      var doc = win.document;

      // If there's a hash, or addEventListener is undefined, stop here
      if (
        !win.navigator.standalone &&
        !location.hash &&
        win.addEventListener
      ) {
        //scroll to 1
        win.scrollTo(0, 1);
        var scrollTop = 1,
          getScrollTop = function () {
            return (
              win.pageYOffset ||
              (doc.compatMode === "CSS1Compat" &&
                doc.documentElement.scrollTop) ||
              doc.body.scrollTop ||
              0
            );
          },
          //reset to 0 on bodyready, if needed
          bodycheck = setInterval(function () {
            if (doc.body) {
              clearInterval(bodycheck);
              scrollTop = getScrollTop();
              win.scrollTo(0, scrollTop === 1 ? 0 : 1);
            }
          }, 15);

        win.addEventListener(
          "load",
          function () {
            setTimeout(function () {
              //at load, if user hasn't scrolled more than 20 or so...
              if (getScrollTop() < 20) {
                //reset to hide addr bar at onload
                win.scrollTo(0, scrollTop === 1 ? 0 : 1);
              }
            }, 0);
          },
          false
        );
      }
    })(this);
  </script>
</head>

<body>

  <section id="intro">
    <div class="container">
      <div >
        <div id="tab-header" class="align-center">
          <center><img src="../assets/images/awards.jpg" width=30%/><br/>
          <h3 class="title no-margin title1">EgoVis 2022/2023 Distinguished Paper Awards</h3>
            <h4>Call for Nominations<br/>Deadline: 1st of May 2024</h4></center>
        </div>
      </div>
    </div>
  </section>


  <section id="cfp">
    <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
       <h4>Summary and Motivation: </h4>

<p>Egocentric vision is gathering huge momentum. One of the objectives of this scheme is to identify, recognise and include recent published works that advance egocentric vision. This will assist in the formation of a solid community as well as increase the profile of the area. 
While egocentric vision papers are now being selected in top conferences as orals, highlights and even paper awardees, the area is still exploratory in nature. Recognising a selective set of outstanding works (up to 10) every year, and introducing such efforts to the community is one way to highlight the latest and greatest works and will form the main task of the EgoVis Board.</p>
        </div></div>
      
  <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
<h4>Eligibility Criteria:</h4>

          <ul>
<li>In this first round, we invite peer-reviewed papers (at conferences or journals) that were accepted between Jan 2022 and Dec 2023. We consider the date of formal acceptance (without any revisions), rather than the conference commencement date or date of proceedings. For conferences, this round will consider papers accepted at: CVPR’22, ECCV’22, NeurIPS’22, ICLR’23, CVPR’23, ICCV’23, NeurIPS’23, ICLR’24. This is a non-exclusive list of venues, and other venues in graphics, robotics or HCI that contribute to egocentric vision will also be considered.</li>
<li>Submissions to journals should be original contributions and are not extensions of a prior conference publication. </li>
<li>Evidence of paper’s acceptance is captured through an email or through a link to the published proceedings (if already published).</li>
<li>Papers from both academia and industry will be considered. There are no additional constraints on authorship, and no maximum number of nominations per author.</li>
</ul>
      </div></div>
      
        <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
          
<h4>Process:</h4>

<p>We invite <a href="https://forms.gle/RV5so9rP5apdkrYv9">self nominations through a form, to be filled by the authors themselves</a>. We invite papers that have already gone through peer-reviewing (i.e. accepted in a major conference or journal of relevance). </p>

<p>The selection process will focus on the original and innovative contribution of the egocentric vision research, as the paper’s prime focus. We strongly encourage papers that also offer open contributions to the community (code, models and data). The board reserves the right to consider distinguished papers that have not been nominated by the authors should they feel a paper deserves to be nominated. The board also reserves the right to recruit reviewers for the assessment of submissions and overall ranking. The reviewing will be single-blind reserving the anonymity of reviewers. Individual board members will recuse themselves from discussions related to papers in which they are a co-author or are otherwise in-conflict with any of the authors through institutions, close collaboration or advisor-student relationships.</p>

          <p>The <a href="https://forms.gle/RV5so9rP5apdkrYv9">nomination form</a> will request not only the accepted manuscript but also additional context (reviews, meta-reviews, published code, …) to assist with the selection process. Additionally, a prologue by the authors on the paper’s standing, current or potential impact, will be considered. Letters of recommendations from other members of the community are not needed and will not be considered.</p>
            </div></div>
        <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
<h4><b>Dates:</b></h4>

<p>The scheme will be open for submissions on 20th of Feb 2024, with nominations accepted until <underline>1st of May 2024</underline>. Winners will be announced at the <a href="https://egovis.github.io/cvpr24/">1st EgoVis workshop alongside CVPR on the 17th of June 2024.</a></p>

<p>Each winning paper will receive a certificate, monetary award of <b>$1000</b> and a highlight on the EgoVis website and through social media. </p>
            </div></div>
        <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
<h4><b>Organising Committee:</b></h4>

<p>The organising EgoVis board represents a diverse set of volunteers who have previously contributed to organising workshops and challenges in Egocentric Vision. Their main motivation, as listed above, is to promote and highlight the latest in the field through this scheme.</p>

EgoVis Board Members (alphabetically)<br/>
<a href="https://homes.luddy.indiana.edu/djcran/">David Crandall</a> - Indiana University<br/>
<a href="https://dimadamen.github.io/">Dima Damen</a> - University of Bristol and Google DeepMind<br/>
<a href="https://www.dmi.unict.it/farinella/">Giovanni Maria Farinella</a> - University of Catania and Next Vision<br/>
<a href="https://www.cs.utexas.edu/users/grauman/">Kristen Grauman</a> - Meta and UT Austin<br/>
<a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a> - UC Berkeley and Meta<br/>
<a href="https://www.linkedin.com/in/richard-newcombe-a094164">Richard Newcombe</a> - Surreal Team, Meta<br/>
<a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a> - Microsoft and ETH Zurich<br/>
<a href="https://sites.google.com/ut-vision.org/ysato/">Yoichi Sato</a> - University of Tokyo
            </div></div>
      
        <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
          <br/><br/><br/>
          <p>Copyright, EgoVis Board, 2024</p>


      </div>
      </div>
    </section>

</body>
<script>

  // Add code snippet for abstract toggle (https://www.w3schools.com/howto/howto_js_collapsible.asp)
  var coll = document.getElementsByClassName("collapsible");
  var i;

  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function () {
      console.log(i);
      this.classList.toggle("active");
      var content = this.nextElementSibling;
      if (content.style.display === "block") {
        content.style.display = "none";
      } else {
        content.style.display = "block";
      }
    });
  }

  // Setup nav-item on click to toggle the hamburger
  const navbar = document.getElementById("nav-bar");
  const hamburger_button = navbar.querySelector("button.navbar-toggler");
  const navlist = navbar.querySelectorAll("a.nav-link").forEach((el) => {
    el.addEventListener("click", () => {
      if (hamburger_button.ariaExpanded === "true") {
        hamburger_button.click();
      }
    });
  });
  const parallax = document.getElementById("parallax-image");
  const tooltip = document.getElementById("parallax-tooltip");
  const video = document.getElementById("parallax-video");
  const [enable, disable] = map_hover(tooltip, video, parallax);

  // Intersection observer to transition header and disable onmousemove and disable tooltip
  let intersection_options = {
    root: null, // viewport
    rootMargin: "0px", // No margin
    threshold: [0.4, 0.2],
  };

  let previousThreshold = -1;
  let observer = new IntersectionObserver((entries, observer) => {
    entries.forEach((_entry) => {
      if (!_entry.isIntersecting) {
        // If it is not intersecting, disable it.
        disable();
        previousThreshold = -1;
        return;
      }

      // If intersecting, then it is either scrolling down or scrolling up.
      if (previousThreshold === -1) {
        previousThreshold = _entry.intersectionRatio;
        return;
      }

      if (_entry.intersectionRatio > previousThreshold) {
        // Going up
        enable();
      } else {
        // Going down
        disable();
      }
      previousThreshold = -1;
    });
  }, intersection_options);
  observer.observe(parallax);

  // Setup map hover
  enable();
</script>

</html>
