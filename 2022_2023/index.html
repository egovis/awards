<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html;charset=UTF-8" />
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description"
    content="EgoVis" />
  <meta name="keywords" content="video dataset, machine perception, machine-learning, research, egocentric videos, ego4d, epic-kitchens" />

  <title>EgoVis 2022/2023 Distinguished Paper Awards</title>
  <link rel="preconnect" href="https://use.fontawesome.com" />
  <link rel="preconnect" href="https://maxcdn.bootstrapcdn.com" />
  <link rel="preconnect" href="https://ajax.googleapis.com" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" />
  <link rel="preconnect" href="https://assets.ego4d-data.org" />

  <link rel="stylesheet" href="../../assets/css/style.css" />

  <link rel="preload" as="image" href="../../assets/images/ego-4d-01.png" />
  <link rel="preload" as="image" href="../--/assets/images/map.jpg"
    media="screen and (max-width: 1024px), (max-height:511px)" />

  <link rel="icon" href="../../assets/images/egovis-logo.ico" />

  <!-- Latest font-awesome css for icons -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css"
    integrity="sha384-DyZ88mC6Up2uqS4h/KRgHuoeGwBcD4Ng9SiP4dIRy0EXTlnuz47vAwmeGwVChigm" crossorigin="anonymous" />

  <!-- BS3 compiled and minified CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" />

  <!-- jQuery library -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <!-- Latest compiled JavaScript for BS3 -->
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

  <script src="../../assets/scripts/form-submission.js"></script>

  <script type="text/javascript">
    function openTabbedContent(element) {
      var whereTo = $(element).attr("goto"),
        tabs = $("#benchmarks li"),
        links = tabs.find("a[href='#" + whereTo + "']");

      links[0].click();
    }
    // Hide the address bar
    // https://24ways.org/2011/raising-the-bar-on-mobile/
    // https://github.com/scottjehl/Hide-Address-Bar
    /*
     * Normalized hide address bar for iOS & Android
     * (c) Scott Jehl, scottjehl.com
     * MIT License
     */
    (function (win) {
      var doc = win.document;

      // If there's a hash, or addEventListener is undefined, stop here
      if (
        !win.navigator.standalone &&
        !location.hash &&
        win.addEventListener
      ) {
        //scroll to 1
        win.scrollTo(0, 1);
        var scrollTop = 1,
          getScrollTop = function () {
            return (
              win.pageYOffset ||
              (doc.compatMode === "CSS1Compat" &&
                doc.documentElement.scrollTop) ||
              doc.body.scrollTop ||
              0
            );
          },
          //reset to 0 on bodyready, if needed
          bodycheck = setInterval(function () {
            if (doc.body) {
              clearInterval(bodycheck);
              scrollTop = getScrollTop();
              win.scrollTo(0, scrollTop === 1 ? 0 : 1);
            }
          }, 15);

        win.addEventListener(
          "load",
          function () {
            setTimeout(function () {
              //at load, if user hasn't scrolled more than 20 or so...
              if (getScrollTop() < 20) {
                //reset to hide addr bar at onload
                win.scrollTo(0, scrollTop === 1 ? 0 : 1);
              }
            }, 0);
          },
          false
        );
      }
    })(this);
  </script>
</head>

<body>

    
      <section id="intro">
    <div class="container">
      <div >
        <div id="tab-header" class="align-center">
          <center><img src="../assets/images/awards.jpg" width=30%/><br/>
          <h3 class="title no-margin title1">EgoVis 2022/2023 Distinguished Paper Awards</h3>
            <h2>List of Winners</h2></center>
        </div>
        <div style="text-align: left; font-size: 14pt">
            <b>The board selected 10 distinguished papers, presented below alphabetically, in no particular order. Official awards were presented during the <a href="https://egovis.github.io/cvpr24/#program">EgoVis workshop @CVPR on the 17th of June.</a></b>
            
            <h2>Acceptance Speeches Video</h2>
            <center>
            <iframe width="840" height="473" src="https://www.youtube.com/embed/YbGWzAw_uVU?si=26i4w6eZulgU0RVV" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></center>
            
            <ul style="line-height: 24pt; margin-top: 12pt; margin-bottom: 24pt; padding: 16pt">
                <li style="margin-bottom: 12pt"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sener_Assembly101_A_Large-Scale_Multi-View_Video_Dataset_for_Understanding_Procedural_Activities_CVPR_2022_paper.pdf">Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities</a>. Fadime Sener, Dibyadip Chatterjee, Daniel Shelepov, Kun He, Dipika Singhania, Robert Wang, Angela Yao. CVPR 2022</li>
	
                <li style="margin-bottom: 12pt"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2023_paper.pdf">Ego-Body Pose Estimation via Ego-Head Pose Estimation</a>. Jiaman Li, C. Karen Liu, Jiajun Wu. CVPR 2023</li>

                <li style="margin-bottom: 12pt"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.html">Ego4D: Around the World in 3,000 Hours of Egocentric Video</a>. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina González, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jáchym Kolář, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbeláez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, Jitendra Malik. CVPR 2022</li>

                <li style="margin-bottom: 12pt"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/31fb284a0aaaad837d2930a610cd5e50-Paper-Conference.pdf">Egocentric Video-Language Pretraining</a>. Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao, Weijie Kong, Chengfei Cai, Hongfa Wang, Dima Damen, Bernard Ghanem, Wei Liu, and Mike Zheng Shou. NeurIPS 2022</li>	

                <li style="margin-bottom: 12pt"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Khirodkar_Ego-Humans_An_Ego-Centric_3D_Multi-Human_Benchmark_ICCV_2023_paper.pdf">EgoHumans: An Egocentric 3D Multi-Human Benchmark</a>. Rawal Khirodkar, Aayush Bansal, Lingni Ma, Richard Newcombe, Minh Vo, Kris Kitani. ICCV 2023</li>

                <li style="margin-bottom: 12pt"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Pramanick_EgoVLPv2_Egocentric_Video-Language_Pre-training_with_Fusion_in_the_Backbone_ICCV_2023_paper.html">EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone</a>. Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou, Rama Chellappa, Pengchuan Zhang. ICCV 2023</li>
	
                <li style="margin-bottom: 12pt"><a href="https://ieeexplore.ieee.org/abstract/document/10096198">EPIC-SOUNDS: A large-scale dataset of actions that sound</a>. Jaesung Huh, Jacob Chalk, Evangelos Kazakos, Dima Damen, Andrew Zisserman. ICASSP 2023</li>	

                <li style="margin-bottom: 12pt"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.pdf">HoloAssist: An Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World</a>. Xin Wang, Taein Kwon, Mahdi Rad, Bowen Pan, Ishani Chakraborty, Sean Andrist, Dan Bohus, Ashley Feniello, Bugra Tekin, Felipe Vieira Frujeri, Neel Joshi, Marc Pollefeys. ICCV 2023</li>

                <li style="margin-bottom: 12pt"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Learning_Video_Representations_From_Large_Language_Models_CVPR_2023_paper.pdf">Learning Video Representations from Large Language Models</a>. Yue Zhao, Ishan Misra, Philipp Krähenbühl, Rohit Girdhar. CVPR 2023</li>

                <li style="margin-bottom: 12pt"><a href="https://openreview.net/forum?id=ChWo6qLgILf">SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning</a>. Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip W Robinson, Kristen Grauman. NeurIPS 2022</li>
            </ul>
            <hr/>
        </div>
      </div>
    </div>
  </section>
    
  <section id="intro">
    <div class="container">
      <div>
        <div id="tab-header" class="align-center">
            <h2><strike>Call for Nominations</strike><br/><strike>Deadline: 1st of May 2024</strike></h2>
        </div>
      </div>
    </div>
  </section>


  <section id="cfp">
    <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
       <h3>Summary and Motivation: </h3>

<p>Egocentric vision is gathering huge momentum. One of the objectives of this scheme is to identify, recognise and include recent published works that advance egocentric vision. This will assist in the formation of a solid community as well as increase the profile of the area. 
While egocentric vision papers are now being selected in top conferences as orals, highlights and even paper awardees, the area is still exploratory in nature. Recognising a selective set of outstanding works (up to 10) every year, and introducing such efforts to the community is one way to highlight the latest and greatest works and will form the main task of the EgoVis Board.</p>
        </div></div>
      
  <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
<h3>Eligibility Criteria:</h3>

          <ul>
<li>In this first round, we invite peer-reviewed papers (at conferences or journals) that were accepted between Jan 2022 and Dec 2023. We consider the date of formal acceptance (without any revisions), rather than the conference commencement date or date of proceedings. For conferences, this round will consider papers accepted at: CVPR’22, ECCV’22, NeurIPS’22, ICLR’23, CVPR’23, ICCV’23, NeurIPS’23, ICLR’24. This is a non-exclusive list of venues, and other venues in graphics, robotics or HCI that contribute to egocentric vision will also be considered.</li>
<li>Submissions to journals should be original contributions and are not extensions of a prior conference publication. </li>
<li>Evidence of paper’s acceptance is captured through an email or through a link to the published proceedings (if already published).</li>
<li>Papers from both academia and industry will be considered. There are no additional constraints on authorship, and no maximum number of nominations per author.</li>
</ul>
      </div></div>
      
        <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
          
<h3>Process:</h3>

<p>We invite <a href="https://forms.gle/RV5so9rP5apdkrYv9">self nominations through a form, to be filled by the authors themselves</a>. We invite papers that have already gone through peer-reviewing (i.e. accepted in a major conference or journal of relevance). </p>

<p>The selection process will focus on the original and innovative contribution of the egocentric vision research, as the paper’s prime focus. We strongly encourage papers that also offer open contributions to the community (code, models and data). The board reserves the right to consider distinguished papers that have not been nominated by the authors should they feel a paper deserves to be nominated. The board also reserves the right to recruit reviewers for the assessment of submissions and overall ranking. The reviewing will be single-blind reserving the anonymity of reviewers. Individual board members will recuse themselves from discussions related to papers in which they are a co-author or are otherwise in-conflict with any of the authors through institutions, close collaboration or advisor-student relationships.</p>

          <p>The <a href="https://forms.gle/RV5so9rP5apdkrYv9">nomination form</a> will request not only the accepted manuscript but also additional context (reviews, meta-reviews, published code, …) to assist with the selection process. Additionally, a prologue (1-pager, any template) by the authors on the paper’s standing, current or potential impact, will be considered. Letters of recommendations from other members of the community are not needed and will not be considered.</p>
            </div></div>
        <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
<h3>Dates:</h3>

<p>The scheme will be open for submissions on 20th of Feb 2024, with nominations accepted until <underline>1st of May 2024</underline>. Winners will be announced at the <a href="https://egovis.github.io/cvpr24/">1st EgoVis workshop alongside CVPR on the 17th of June 2024.</a></p>

<p>Each winning paper will receive a certificate, monetary award of <b>$1000</b> and a highlight on the EgoVis website and through social media. </p>
            </div></div>
        <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
<h3>Organising Committee:</h3>

<p>The organising EgoVis board represents a diverse set of volunteers who have previously contributed to organising workshops and challenges in Egocentric Vision. Their main motivation, as listed above, is to promote and highlight the latest in the field through this scheme.</p>

EgoVis Board Members (alphabetically)<br/>
<a href="https://homes.luddy.indiana.edu/djcran/">David Crandall</a> - Indiana University<br/>
<a href="https://dimadamen.github.io/">Dima Damen</a> - University of Bristol and Google DeepMind<br/>
<a href="https://www.dmi.unict.it/farinella/">Giovanni Maria Farinella</a> - University of Catania and Next Vision<br/>
<a href="https://www.cs.utexas.edu/users/grauman/">Kristen Grauman</a> - Meta and UT Austin<br/>
<a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a> - UC Berkeley and Meta<br/>
<a href="https://www.linkedin.com/in/richard-newcombe-a094164">Richard Newcombe</a> - Meta Reality Labs<br/>
<a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a> - Microsoft and ETH Zurich<br/>
<a href="https://sites.google.com/ut-vision.org/ysato/">Yoichi Sato</a> - University of Tokyo
            </div></div>
      
        <div class="container">
      
      <div style="text-align: left; font-size: 12pt">
          <br/><br/><br/>
          <p>Copyright, EgoVis Board, 2024</p>


      </div>
      </div>
    </section>

</body>
<script>

  // Add code snippet for abstract toggle (https://www.w3schools.com/howto/howto_js_collapsible.asp)
  var coll = document.getElementsByClassName("collapsible");
  var i;

  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function () {
      console.log(i);
      this.classList.toggle("active");
      var content = this.nextElementSibling;
      if (content.style.display === "block") {
        content.style.display = "none";
      } else {
        content.style.display = "block";
      }
    });
  }

  // Setup nav-item on click to toggle the hamburger
  const navbar = document.getElementById("nav-bar");
  const hamburger_button = navbar.querySelector("button.navbar-toggler");
  const navlist = navbar.querySelectorAll("a.nav-link").forEach((el) => {
    el.addEventListener("click", () => {
      if (hamburger_button.ariaExpanded === "true") {
        hamburger_button.click();
      }
    });
  });
  const parallax = document.getElementById("parallax-image");
  const tooltip = document.getElementById("parallax-tooltip");
  const video = document.getElementById("parallax-video");
  const [enable, disable] = map_hover(tooltip, video, parallax);

  // Intersection observer to transition header and disable onmousemove and disable tooltip
  let intersection_options = {
    root: null, // viewport
    rootMargin: "0px", // No margin
    threshold: [0.4, 0.2],
  };

  let previousThreshold = -1;
  let observer = new IntersectionObserver((entries, observer) => {
    entries.forEach((_entry) => {
      if (!_entry.isIntersecting) {
        // If it is not intersecting, disable it.
        disable();
        previousThreshold = -1;
        return;
      }

      // If intersecting, then it is either scrolling down or scrolling up.
      if (previousThreshold === -1) {
        previousThreshold = _entry.intersectionRatio;
        return;
      }

      if (_entry.intersectionRatio > previousThreshold) {
        // Going up
        enable();
      } else {
        // Going down
        disable();
      }
      previousThreshold = -1;
    });
  }, intersection_options);
  observer.observe(parallax);

  // Setup map hover
  enable();
</script>

</html>
